### Детский мир (Август 2022 — Май 2025, 2 года 10 месяцев)

В 'Детском мире' я работал Golang-разработчиком в **команде Платформы лояльности и персонализированного маркетинга**. Это была очень динамичная среда, характерная для крупного ритейла, где постоянно появляются новые акции и требуется быстрая адаптация систем. Мы работали по **Scrum**, обычно с двухнедельными спринтами.

### Как была устроена командная работа и процессы:

**Команда:** Наша продуктовая команда состояла из 6-7 бэкенд-разработчиков на Go (включая меня), 3 QA-инженеров, выделенного DevOps-инженера (частично он работал и на другие команды), бизнес-аналитика и тимлида. У нас была смешанная модель работы – часть команды работала из офиса, часть удаленно, поэтому коммуникация в основном шла через корпоративный мессенджер и видеоконференции.

**Scrum-церемонии:**

- **Планирование спринта:** В начале спринта мы собирались, чтобы декомпозировать задачи из бэклога, обсуждали их техническую реализацию и оценивали в сторипоинтах (использовали шкалу Фибоначчи). Это занимало до двух часов.
    
- **Дейли:** Каждое утро был короткий дейли-митинг на 10-15 минут, где мы синхронизировались по статусу задач, делились планами и озвучивали блокеры.
    
- **Груминг бэклога:** Раз в неделю или раз в две недели проводили груминг бэклога, чтобы убедиться, что задачи на следующие спринты достаточно детализированы и понятны.
    
- **Ретроспектива:** В конце спринта обсуждали, что удалось, что можно улучшить в процессе работы и взаимодействии внутри команды.
    

**Коммуникация:** Задачи и баги отслеживали в **Jira**, всю документацию вели в **Confluence**. Для оперативного общения и решения вопросов использовали корпоративный мессенджер. Код-ревью было обязательным: каждый Pull Request рассматривали как минимум два коллеги, что помогало поддерживать высокое качество кода и обмениваться знаниями. С QA мы работали очень плотно: обсуждали тест-кейсы, проводили совместные сессии по тестированию.

### Проект и мои задачи:

Основной проект, над которым мы работали, – это развитие и поддержка **программы лояльности 'Детского мира'**. Это высоконагруженная система, которая обслуживает миллионы клиентов и тысячи транзакций ежедневно, с частыми акциями и сложными правилами начисления бонусов. Требования к производительности, надежности и консистентности данных были очень высокими, так как любая ошибка напрямую влияла на клиентский опыт и бизнес-показатели.

**Что конкретно делал:**

- Одной из ключевых задач была **разработка нового модуля на Go для расчета и начисления бонусных баллов**. Этот модуль учитывал сложные правила акций, сегменты клиентов и различные типы покупок. Я отвечал за его архитектуру, реализацию и интеграцию с основной базой данных программы лояльности на **PostgreSQL**. Модуль должен был быть отказоустойчивым и обеспечивать высокую точность расчетов даже при пиковых нагрузках, поэтому особое внимание уделял идемпотентности и транзакционности операций.
    
- Активно участвовал в **проектировании и реализации REST API для мобильного приложения "Детский мир"**. Этот API предоставлял пользователям доступ к информации о балансе баллов, истории покупок и персональным купонам. Я разрабатывал эндпоинты, занимался валидацией запросов и формированием ответов, обеспечивая быстрый и безопасный доступ к данным лояльности.
    
- Занимался **оптимизацией процессов синхронизации данных о клиентах и их транзакциях** между онлайн-платформой и офлайн-магазинами. Для этого использовались **Kafka**-топики: данные о покупках из офлайн-касс публиковались в Kafka, а наши Go-сервисы их потребляли, обрабатывали и обновляли информацию в основной базе данных. Это было критически важно для обеспечения консистентности балансов и актуальности предложений для клиентов.
    
- **Внедрил использование Redis для кэширования** часто запрашиваемых данных программы лояльности, таких как текущие акции, сегментация клиентов и баланс баллов. Это позволило значительно снизить нагрузку на PostgreSQL и сократить время отклика API примерно на 40% для кэшированных запросов, что мы отслеживали по графикам в **Grafana**.
    
- **Настроил и поддерживал мониторинг (Prometheus, Grafana)** для ключевых сервисов программы лояльности. Я разрабатывал кастомные метрики для Go-приложений, отслеживал RPS, P99 latency, количество ошибок, глубину очередей Kafka, алерты по этим метрикам. Также использовал **ELK Stack** для агрегации и анализа логов, что помогало оперативно находить и устранять проблемы.
    
- Активно работал с **Docker** для контейнеризации наших Go-приложений и **Kubernetes** для их развертывания в облачной инфраструктуре. Писал **Helm**-чарты, настраивалingress-контроллеры и секреты, обеспечивая бесшовное развертывание и масштабирование сервисов.
    
- Проводил **рефакторинг и оптимизацию существующих сервисов** программы лояльности. Например, перерабатывал некоторые SQL-запросы в PostgreSQL, которые вызывали блокировки на высоконагруженных таблицах, что приводило к деградации производительности. Это позволило увеличить пропускную способность системы и снизить среднее время ответа.
    

Интересная задача, которая запомнилась:

Одной из самых интересных задач была разработка динамической системы применения скидок и купонов. Изначально логика была довольно жесткой, но бизнес требовал возможности быстро запускать новые, очень гибкие акции. Мы спроектировали и реализовали систему на Go, которая позволяла маркетологам задавать сложные условия (например, "скидка 10% на детскую одежду при покупке от 3000 рублей, если клиент имеет статус 'Премиум' и совершил покупку в течение последних 7 дней"). Это потребовало создания гибкого движка правил, который мог бы оперативно реагировать на изменения в данных клиента и его корзины, а также эффективно работать с кэшем.

Про фейл, который научил:

Однажды, при развертывании новой версии сервиса, отвечающего за авторизацию, мы столкнулись с неожиданными пиками ошибок 5xx. Оказалось, что изменение в настройках соединения с базой данных (pool size) в новом релизе, которое выглядело некритичным на тестовых стендах, в проде приводило к исчерпанию лимитов соединений под высокой нагрузкой. Мы быстро откатили версию. Этот инцидент подчеркнул, насколько важно тщательно тестировать все изменения конфигурации под нагрузкой, максимально приближенной к продуктовой, и иметь надежный механизм мониторинга, способный моментально сигнализировать о любых аномалиях.

---

## Grow Food (Март 2021 — Июль 2022, 1 год 5 месяцев)

"В Grow Food я работал Golang Developer в **команде Производственной и Логистической платформы**. Это была динамичная среда стартапа в сфере доставки готовой еды, где скорость и эффективность логистических процессов напрямую влияли на бизнес. Мы работали в гибкой методологии, чаще всего это был **Kanban** или упрощенный Scrum, так как важно было быстро адаптироваться к меняющимся требованиям бизнеса.

### Как была устроена командная работа и процессы:

**Команда:** Моя команда обычно состояла из 2-3 бэкенд-разработчиков на Go (я был одним из ключевых Go-разработчиков), 1-2 фронтендера, QA-инженера и Project-менеджера. Команда была довольно компактной и сфокусированной на конкретных доменах — производственных или логистических процессах. Взаимодействие было тесным и непосредственным.

**Методология:** В зависимости от проекта, мы использовали либо Kanban для потоковой разработки, либо Scrum с короткими недельными или двухнедельными спринтами. Гибкость была ключевым фактором, так как бизнес-требования часто менялись.

**Планирование и оценка:** Задачи поступали от Project-менеджера или от бизнес-аналитиков. Мы их обсуждали, декомпозировали и оценивали в часах или Story Points, исходя из сложности и ожидаемого объема работы.

**Дейли и коммуникация:** Ежедневные короткие стендапы (дейли-митинги) были стандартом. Основная коммуникация шла через Slack и Zoom. Для трекинга задач использовали **Jira** и **Confluence** для документации. Код-ревью было неотъемлемой частью процесса разработки, что помогало поддерживать качество и делиться знаниями внутри команды.

### Проекты и мои задачи:

Моя основная задача заключалась в разработке и поддержке ключевых сервисов, лежащих в основе производственной и логистической цепочки Grow Food. Это включало планирование производства, управление складскими запасами, распределение заказов и оптимизацию маршрутов доставки. Системы должны были быть высокодоступными и быстро реагировать на изменения в реальном времени.

**Что конкретно делал:**

- Разработал несколько **сервисов на Go для планирования производственных заданий на кухне**. Эти сервисы анализировали поступившие заказы, учитывали прогноз спроса и текущие остатки ингредиентов, а затем формировали оптимальные задания для поваров. Интеграция с другими системами происходила через **RabbitMQ**, что обеспечивало асинхронную и надежную передачу данных.
    
- Участвовал в **создании системы управления складскими остатками ингредиентов и готовой продукции**. Это включало разработку **API (REST)** для мобильного приложения кладовщиков, через которое они могли принимать, отгружать и инвентаризировать продукцию. Я отвечал за логику работы с запасами, учет сроков годности и формирование отчетов по движению товаров.
    
- Реализовал **модуль для автоматического распределения заказов по курьерам и оптимизации маршрутов доставки**. Этот модуль учитывал множество параметров: временные окна доставки клиента, загруженность курьеров, их текущее местоположение, тип транспортного средства. Он взаимодействовал с внешней системой маршрутизации и предоставлял курьерам оптимальный путь через их мобильное приложение. Для этого использовались gRPC-сервисы для внутреннего взаимодействия.
    
- **Интегрировал платформу с внешними картографическими сервисами**. Это позволило в реальном времени отслеживать местоположение курьеров, отображать актуальную информацию о статусе доставки для клиентов и службы поддержки. Я занимался парсингом и обработкой данных о геопозиции, а также обеспечением их отображения на картах.
    
- **Оптимизировал работу с базой данных PostgreSQL**. В ней хранилась вся критически важная информация о заказах, клиентах, рецептурах, складских операциях и логистике. Я работал над оптимизацией SQL-запросов, созданием индексов, проектированием схем баз данных для обеспечения высокой производительности и масштабируемости. Иногда использовал **TimescaleDB** для хранения временных рядов данных, например, для истории перемещений курьеров.
    
- Разработал **внутренний инструмент для службы поддержки**, который позволял операторам оперативно отслеживать статус заказа, просматривать историю доставки, а также вносить необходимые корректировки в случае непредвиденных ситуаций. Это был Go-сервис с простым REST API, который агрегировал данные из нескольких источников.
    

Интересная задача, которая запомнилась:

Одним из самых интересных вызовов было создание системы динамического распределения заказов на основе текущей ситуации с курьерами и дорожной обстановкой. Изначально распределение было более статичным, но с ростом числа заказов требовалась более сложная логика. Мы использовали Go для разработки сервиса, который непрерывно обрабатывал потоки данных из различных источников (новые заказы, статус курьеров, данные о трафике от внешних API) и в реальном времени перераспределял задания, чтобы минимизировать время доставки и оптимизировать загрузку курьеров. Это был отличный опыт работы с real-time данными и алгоритмами оптимизации.

Про фейл, который научил:

На старте одного из проектов мы столкнулись с проблемой, когда некорректно оценили объем данных, которые будут храниться в базе для отслеживания истории перемещений курьеров. Изначально мы использовали обычный PostgreSQL, но очень быстро объем данных стал критическим, что привело к замедлению запросов и проблемам с производительностью. Урок был в том, что необходимо заранее проводить тщательное прогнозирование объемов данных и выбирать подходящие инструменты. В итоге мы перешли на TimescaleDB, что решило проблему с хранением и анализом временных рядов, но это потребовало значительных усилий по миграции и рефакторингу.

---
